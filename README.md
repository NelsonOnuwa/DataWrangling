Data

Data is a collection of raw facts and figures that we need to process to extract meaning or information
Data can be any number (0 to 9), characters (A to Z, a to z), text, words, statements, special characters (*, /, @, #, etc), pictures, sound, or videos that are context less, meaning little or nothing to a human being.

For Example: Will Turner, 48, link down, blue, junior, ocean, street

The data is raw and there is no meaning to it but if we organize this data:

Will tuner

H.no - 48, blue ocean,

Link down street,

Now, this looks like an Address of the person named Will Turner. Whereas, in the above example it is impossible to make out the meaning of the words.

Data is nothing unless it is processed or is aligned in some context. The data when structured and organized (when the data is processed in some manner) results in Information.

Information is “ processed data that is organized, structured or presented in a given context so that the data deliver some logical meaning that may be further utilized in decision making”.
Data is something that you can consider as a low level of knowledge where you have some scattered, uncategorized, unorganized entities that do not really mean anything. Whereas Information is the second level of knowledge, where you wire up the data and assign it some context so, the data becomes meaningful.







Data science


Data science is not python,R,SQL,statistics or data visualization.

Data science is applying tools to analyse data in order to extract useful information.

There are four jobs within data science:
    
      job                                              Roles                                  Tools
    
    Data Engineer                            store & maintain data                     SQL/Python
    Data Analyst                             visualise & describe data                 SQL/Bi tools
    Data Scientist                           Gain insights from data                   Python
    Machine learning(ML)Scientist            Predict with data                         Python
   

Data scientists are responsible for collectting and analyzing complex data sets to identify patterns and insights that inform business strategy. A data scientist role is critical to any organization looking to leverage their data assets for competitive advantage.

He/she communicates the significance of the data a business holds in a way that others can comprehend.

To achieve this, a data scientist needs to have a working knowledge of the many programming languages and data science methodologies for addressing this challenging task.They must possess a deep understanding of statistical models, machine learning and data mining techniques.











Data Format


There are the four (4) data formats commonly seen in data processing, they include structured, semi structures, quasi-structured and unstructured data format. 

Structured data is highly organized, easily accessible and retrievable from where it is stored. It is machine-readable.

Semi structured data is partially organized and difficult to retrieve. HTML(Hypertext Markup Language), XML (eXtensible Markup Language) and other markup languages are examples of semi-structured data.

Quasi-structured data involves specifically textual data that has a temporal state and having erratic data formats. Data streams on a social media page or clickstream data from Google searches are examples of quasi-structured data.

Unstructured data is not organized, it is difficult to access and to retrieve from where it is stored. Data from social media posts, emails, and chats are examples of unstructured data. Unstructured data could be stored using XML, JSON (JavaScript Object Notation), or CSV (Comma-Separated Values) file formats and It is human-readable.



Data Types


String, Integers, float and boolean are some common examples of data type.











Big Data 


Big Data defines a situation in which data sets have grown to such enormous sizes that conventional information technologies can no longer effectively handle the size of the data set. Big data grows exponentially (becoming more rapid) and is boundless (volume), has varied formats (variety), and has high complexity (voracity and velocity). The four Vs of big data: volume, variety, voracity and velocity.
Big data emerges from a variety of sources comprising of Internet of things (IoTs), factories, enterprise resource planning and customer relations management systems according to industry experts.








Big Data Security


Many big data tools used by enterprises to identify business opportunities, improve performance, and drive decision-making are open source and not designed with security in mind. This huge increase in data consumption leads to many data security concerns, which include Information theft, distributed denial-of-service (DDoS), ransomware etc.
It is difficult to secure big data because of constant access by different users, the presence of open source tools and the multiple feeds of data from sources with different protection needs. Big data security challenges can be experienced both on-premises and in the cloud. It is prevalent in non-relational database, endpoint vulnerabilities, data mining solutions, access distributed data processing and storage task.

Data security threats can he handled using security techniques such as, user access centralized key management, encryption, and intrusion detection and prevention (IDPS). One way that organizations can protect data is through encryption, which applies algorithms to scramble data so that it is readable only by someone who holds the key to decrypt it. Encryption takes a piece of data, commonly called the plaintext, combines it with a cryptographic key. This produces a scrambled version of the data called the cipher text. It is possible to decrypt the data to recover the plaintext using the key, but without the key, the cipher text hides all information about the original data, other than its length.
Using user access control to protect data, we turn to cryptographic techniques to secure data in storage. The primary goal of this technique is to enforce access control to data stored in potentially untrusted repositories. That is, we give authorized parties access to the data they need while ensuring that unauthorized parties, either outsiders trying to gain access or malicious insiders in the organization managing the repository, cannot access sensitive data.
An intrusion detection and prevention system (IDPS) is a solution that monitors network for simpler threats and then takes flag complex threats to alert security teams to stop any threat detected.
Internet of things (IoTs) are physical objects such as our phones, appliances, lighting systems, irrigation systems, security cameras, vehicles and cities equipped with sensors and software, which enable them to interact with each other by collecting and exchanging data via wireless network, with little human intervention. The Internet of things (IoTs) simplifies and automates complicated tasks by uniting the objects under one common infrastructure, transforming them into smart objects and remotely controls them.





Internet of things (IoTs)



Internet of things (IoTs) are physical objects such as our phones, appliances, lighting systems, irrigation systems, security cameras, vehicles and cities equipped with sensors and software, which enable them to interact with each other by collecting and exchanging data via wireless network, with little human intervention.

In the late 1960 and 1970, the inquiry into the idea to connect personal computers (PCs) to other machines began and by 1980s, the invention of local area networks (LANs) provided an effective way to share documents and data across PCs in real time. The invention of the internet by mid 1990s extended LANs capabilities globally and in 1997, a British technologist Kevin Ashton, cofounder of the Auto-ID Center, began working on radio-frequency identification (RFID) technology that would allow physical devices to connect via microchips and wireless signals. During a speech in 1999, Ashton invented the phrase “the Internet of Things”.

The Internet of things (IoTs) simplifies and automates complicated tasks. It unites the objects under one common infrastructure, it transforms them into smart objects and it remotely controls them.

In transportation, the internet of things (IoTs) technologies ensures the proper tracking and delivery of goods from the manufacturer to the end user in real time. This promotes speed, efficiency and reducing personnel costs. It can also provide a medium for checking the integrity of goods through barcode, the location of goods via GPS, the monitoring of parameters and status variables of the assets via sensors and their transmission via Wi-Fi or GSM/GPRS network. Thus preventing losses, ensuring safety in storage of goods and efficiently locating the products needed.

The collection of useful data about food crops and the detection of potential diseases in advance in agriculture, has led to improved agricultural production processes and the ability to meet food demands.

In smart cities, the installation of sensors linked with many other devices over the internet gives information to users about malfunctions, electrical failure and issues with traffic jams, energy supply, water shortage, security incidents, etc. In addition, in smart cars the early detection of potential car failure, tire pressure, fueling needs and regular maintenance increases comfort and upgrade driver’s experience.

The healthcare services can better monitor the heart rate and skin temperature, they can predict different symptoms and prevent potentially life threatening diseases through internet of things (IoTs) technologies.In a global pandemic, fast data collection and diversity is possible.

With internet of things (IoTs) devices, specific challenges ranging from the inadequacy of security protection leading to loss, stolen, or incorrectly used data to the safety concerns in smart cars and and the electronic waste from IoT products. In addition, the insufficient investigation of long-term effects of IoT technologies and its sustainability are bound to have specific potential drawbacks that need careful consideration.






Data Wrangling

Data science and analytics are taking over the whole world and the job of a data scientist is routinely being called the coolest job of the 21st century.

But for all the emphasis on data, it is the science that makes you – the practitioner – truly valuable To practice high-quality science with data, you need to make sure it is properly sourced, cleaned, formatted, and pre-processed This book teaches you the most essential basics of this invaluable component of the data science pipeline: data wrangling.

Data wrangling is the process that ensures that the data is in a format that is clean, accurate, formatted, and ready to be used for data analysis.\n",

After the data scientists identify useful data sources for solving the business problem (for instance, in-house database storage or internet or streaming sensor data), they then proceed to extract, clean, and format the necessary data from those sources. Data wrangling is generally done at the very first stage of a data science/analytics pipeline.

Generally, the task of data wrangling involves the following steps:

Scraping raw data from multiple sources (including web and database tables)
Imputing, formatting, and transforming – basically making it ready to be used in the modeling process (such as advanced machine learning)
Handling read/write errors
Detecting outliers
Performing quick visualizations (plotting) and basic statistical analysis to judge the quality of your formatted data.
The process of data wrangling includes first finding the appropriate data that's necessary for the analysis. This data can be from one or multiple sources, such as tweets, bank transaction statements in a relational database, sensor data, and so on. This data needs to be cleaned. If there is missing data, we will either delete or substitute it, with the help of several techniques. If there are outliers, we need to first detect them and then handle them appropriately. If data is from multiple sources, we will have to perform join operations to combine it.

There is always a debate on whether to perform the wrangling process using an enterprise tool or by using a programming language
and associated frameworks.
    
There are many commercial, enterprise-level tools for data formatting and pre-processing that do not 
involve much coding on the part of the user.
    
These examples include the following:
          General purpose data analysis platforms such as Microsoft Excel (with add-ins)
          Statistical discovery package such as JMP (from SAS),
          Modeling platforms such as RapidMiner,
          Analytics platforms from niche players focusing on data wrangling, such as Trifacta, Paxata, and Alteryx

However, programming languages such as Python provide more flexibility, control, and power compared to these off-the-shelf tools.
As the volume, velocity, and variety (the three Vs of big data) of data undergo rapid changes.
    
it is always a good idea to develop and nurturea significant amount of in-house expertise in data wrangling using 
fundamental programming frameworks so that an organization is not beholden to the whims and fancies of any enterprise platformfor as basic
a task as data wrangling:
When data scientists identify useful data sources for solving the business problem they then proceed to extract, clean, and format the necessary data from those sources. Data wrangling is the process that ensures that the data is in a format that is clean and accurate to yield a data set that is suitable for exploration and analysis. Data wrangling extracts the most valuable information from the dataset.
The process of data wrangling includes first finding the appropriate data that is necessary for the analysis. This data can be from one or multiple sources, such as tweets, bank transaction statements in a relational database, sensor data, and so on. Next, the data is cleaned, where there is missing data, we will either delete or substitute it with the help of several techniques. If there are outliers, we need to first detect them and then handle them appropriately. If data is from multiple sources, we will have to perform join operations to combine it.

The primary challenge of data wrangling stems from the time commitment involved to complete the task and the little amount of automated work that could be carried out on the dataset. Understanding large volume of datasets, the processing of the datasets and combination of different data types possess a significant challenge.
Using the appropriate tools such as python, data wrangler, google data prep, parse hub, excel and talend can overcome the challenges of data wrangling.





References


Sandro, N (2020) Internet of things. Available at: Internet of Things (IoT): Opportunities, issues and challenges towards a smart and sustainable future - ScienceDirect (Accessed: 15 February 2024).
Mori, H (2022) IoT technologies in smart environment: security issues and future enhancement. Available at: IoT technologies in smart environment: security issues and future enhancements - PubMed (nih.gov) (Accessed: 16 February 2024).


Mohammed, T (2019) Smart home based IoT for real time and secure remote health monitoring of triage and priority system using body sensors. Smart Home-based IoT for Real-time and Secure Remote Health Monitoring of Triage and Priority System using Body Sensors: Multi-driven Systematic Review - PubMed (nih.gov) (Accessed: 16 February 2024).


Eur- lex (2016) Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation). Available at: Regulation - 2016/679 - EN - gdpr - EUR-Lex (europa.eu) (Accessed 26 January 2024).


Semarchy (2022) Seven best practices for master data management in banking. Available at: Master Data Management in Banking: 7 Best Practices - Semarchy (Accessed 30 January 2024).
